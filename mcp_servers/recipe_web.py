"""
Morizo AI v2 - Recipe Web Search Module

This module provides web search functionality for recipe retrieval using Google Search API and Perplexity API.
"""

import os
import re
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin
import requests
from googleapiclient.discovery import build
from dotenv import load_dotenv
from config.loggers import GenericLogger
from bs4 import BeautifulSoup

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ–
logger = GenericLogger("mcp", "recipe_web", initialize_logging=False)


class _GoogleSearchClient:
    """Google Search APIã‚’ä½¿ç”¨ã—ãŸãƒ¬ã‚·ãƒ”æ¤œç´¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    # ãƒ¢ãƒƒã‚¯æ©Ÿèƒ½ã®åˆ‡ã‚Šæ›¿ãˆãƒ•ãƒ©ã‚°ï¼ˆèª²é‡‘å›é¿ç”¨ï¼‰
    # ç’°å¢ƒå¤‰æ•° USE_MOCK_SEARCH ã§åˆ¶å¾¡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
    USE_MOCK_SEARCH = os.getenv('USE_MOCK_SEARCH', 'True').lower() in ('true', '1', 'yes')
    
    def __init__(self):
        self.api_key = os.getenv('GOOGLE_SEARCH_API_KEY')
        self.engine_id = os.getenv('GOOGLE_SEARCH_ENGINE_ID')
        
        if not self.api_key or not self.engine_id:
            raise ValueError("GOOGLE_SEARCH_API_KEY and GOOGLE_SEARCH_ENGINE_ID are required")
        
        self.service = build("customsearch", "v1", developerKey=self.api_key)
        
        # å¯¾å¿œã‚µã‚¤ãƒˆã®å®šç¾©
        self.recipe_sites = {
            'cookpad.com': 'Cookpad',
            'kurashiru.com': 'ã‚¯ãƒ©ã‚·ãƒ«',
            'recipe.rakuten.co.jp': 'æ¥½å¤©ãƒ¬ã‚·ãƒ”',
            'delishkitchen.tv': 'ãƒ‡ãƒªãƒƒã‚·ãƒ¥ã‚­ãƒƒãƒãƒ³'
        }
        
        # ãƒ¢ãƒƒã‚¯ç”¨ãƒ¬ã‚·ãƒ”ãƒ‡ãƒ¼ã‚¿ï¼ˆèª²é‡‘å›é¿ç”¨ï¼‰
        self.mock_recipes = [
            {
                'title': 'ç°¡å˜ï¼åŸºæœ¬ã®ãƒãƒ³ãƒãƒ¼ã‚°',
                'url': 'https://cookpad.com/jp/recipes/17546743',
                'description': 'ãµã‚ãµã‚ã§ã‚¸ãƒ¥ãƒ¼ã‚·ãƒ¼ãªãƒãƒ³ãƒãƒ¼ã‚°ã®ä½œã‚Šæ–¹ã€‚åŸºæœ¬ã®ãƒ¬ã‚·ãƒ”ãªã®ã§åˆå¿ƒè€…ã§ã‚‚å®‰å¿ƒã—ã¦ä½œã‚Œã¾ã™ã€‚',
                'site': 'cookpad.com',
                'source': 'Cookpad'
            },
            {
                'title': 'çµ¶å“ï¼ã‚ªãƒ ãƒ©ã‚¤ã‚¹',
                'url': 'https://cookpad.com/jp/recipes/19174499',
                'description': 'ãµã‚ãµã‚ã®åµã§åŒ…ã‚“ã ã‚ªãƒ ãƒ©ã‚¤ã‚¹ã€‚ã‚±ãƒãƒ£ãƒƒãƒ—ãƒ©ã‚¤ã‚¹ã¨åµã®ç›¸æ€§ãŒæŠœç¾¤ã§ã™ã€‚',
                'site': 'cookpad.com',
                'source': 'Cookpad'
            },
            {
                'title': 'æœ¬æ ¼ï¼ã‚«ãƒ¬ãƒ¼ãƒ©ã‚¤ã‚¹',
                'url': 'https://cookpad.com/jp/recipes/19240768',
                'description': 'ã‚¹ãƒ‘ã‚¤ã‚¹ã‹ã‚‰ä½œã‚‹æœ¬æ ¼ã‚«ãƒ¬ãƒ¼ã€‚æ™‚é–“ã‚’ã‹ã‘ã¦ä½œã‚‹ã“ã¨ã§æ·±ã„å‘³ã‚ã„ãŒæ¥½ã—ã‚ã¾ã™ã€‚',
                'site': 'cookpad.com',
                'source': 'Cookpad'
            },
            {
                'title': 'ç°¡å˜ï¼ãƒã‚­ãƒ³ã‚½ãƒ†ãƒ¼',
                'url': 'https://cookpad.com/jp/recipes/17426721',
                'description': 'ã‚¸ãƒ¥ãƒ¼ã‚·ãƒ¼ã§æŸ”ã‚‰ã‹ã„ãƒã‚­ãƒ³ã‚½ãƒ†ãƒ¼ã®ä½œã‚Šæ–¹ã€‚ä¸‹å‘³ãŒãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚',
                'site': 'cookpad.com',
                'source': 'Cookpad'
            },
            {
                'title': 'çµ¶å“ï¼ãƒ‘ã‚¹ã‚¿',
                'url': 'https://cookpad.com/jp/recipes/18584308',
                'description': 'æœ¬æ ¼çš„ãªãƒ‘ã‚¹ã‚¿ã®ä½œã‚Šæ–¹ã€‚ã‚¢ãƒ«ãƒ‡ãƒ³ãƒ†ã®éººã¨ã‚½ãƒ¼ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹ãŒé‡è¦ã§ã™ã€‚',
                'site': 'cookpad.com',
                'source': 'Cookpad'
            },
            {
                'title': 'ç°¡å˜ï¼ã‚µãƒ©ãƒ€',
                'url': 'https://cookpad.com/jp/recipes/17616085',
                'description': 'æ–°é®®ãªé‡èœã‚’ä½¿ã£ãŸã‚µãƒ©ãƒ€ã€‚ãƒ‰ãƒ¬ãƒƒã‚·ãƒ³ã‚°ã®ä½œã‚Šæ–¹ã‚‚ç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚',
                'site': 'cookpad.com',
                'source': 'Cookpad'
            },
            {
                'title': 'çµ¶å“ï¼ã‚¹ãƒ¼ãƒ—',
                'url': 'https://cookpad.com/jp/recipes/17563615',
                'description': 'ä½“ãŒæ¸©ã¾ã‚‹ç¾å‘³ã—ã„ã‚¹ãƒ¼ãƒ—ã€‚é‡èœã®ã†ã¾å‘³ãŒãŸã£ã·ã‚Šã§ã™ã€‚',
                'site': 'cookpad.com',
                'source': 'Cookpad'
            },
            {
                'title': 'ç°¡å˜ï¼ç‚’é£¯',
                'url': 'https://cookpad.com/jp/recipes/17832934',
                'description': 'ãƒ‘ãƒ©ãƒ‘ãƒ©ã§ç¾å‘³ã—ã„ç‚’é£¯ã®ä½œã‚Šæ–¹ã€‚ã‚³ãƒ„ã‚’æ´ã‚ã°ç°¡å˜ã«ä½œã‚Œã¾ã™ã€‚',
                'site': 'cookpad.com',
                'source': 'Cookpad'
            },
            {
                'title': 'çµ¶å“ï¼å¤©ã·ã‚‰',
                'url': 'https://cookpad.com/jp/recipes/17564487',
                'description': 'ã‚µã‚¯ã‚µã‚¯ã§ç¾å‘³ã—ã„å¤©ã·ã‚‰ã®ä½œã‚Šæ–¹ã€‚è¡£ã®ä½œã‚Šæ–¹ãŒãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚',
                'site': 'cookpad.com',
                'source': 'Cookpad'
            },
            {
                'title': 'ç°¡å˜ï¼ç…®ç‰©',
                'url': 'https://cookpad.com/jp/recipes/18558350',
                'description': 'ã»ã£ã“ã‚Šç¾å‘³ã—ã„ç…®ç‰©ã€‚é‡èœã®ç”˜ã¿ãŒå¼•ãå‡ºã•ã‚Œã¾ã™ã€‚',
                'site': 'cookpad.com',
                'source': 'Cookpad'
            }
        ]
    
    async def search_recipes(self, recipe_title: str, num_results: int = 5) -> List[Dict[str, Any]]:
        """ãƒ¬ã‚·ãƒ”æ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆè¤‡æ•°ã‚µã‚¤ãƒˆå¯¾å¿œï¼‰"""
        logger.debug(f"ğŸ” [WEB] Searching recipes")
        logger.debug(f"ğŸ” [WEB] Recipe title: {recipe_title}")
        
        # ãƒ¢ãƒƒã‚¯æ©Ÿèƒ½ãŒæœ‰åŠ¹ãªå ´åˆã¯ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        if self.USE_MOCK_SEARCH:
            logger.debug(f"ğŸ­ [WEB] Using mock data (Google Search API disabled)")
            # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦é–¢é€£ã™ã‚‹ãƒ¬ã‚·ãƒ”ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_recipes = self._filter_mock_recipes(recipe_title, num_results)
            logger.debug(f"âœ… [WEB] Found mock recipes")
            logger.debug(f"ğŸ“Š [WEB] Found {len(filtered_recipes)} mock recipes")
            return filtered_recipes
        
        try:
            # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
            query = self._build_recipe_query(recipe_title)
            
            result = self.service.cse().list(
                q=query,
                cx=self.engine_id,
                num=num_results,
                lr='lang_ja'  # æ—¥æœ¬èªã«é™å®š
            ).execute()
            
            # çµæœã‚’è§£æãƒ»æ•´å½¢
            recipes = self._parse_search_results(result.get('items', []))
            
            logger.debug(f"âœ… [WEB] Found recipes")
            logger.debug(f"ğŸ“Š [WEB] Found {len(recipes)} recipes")
            return recipes
            
        except Exception as e:
            logger.error(f"âŒ [WEB] Search error: {e}")
            return []
    
    def _filter_mock_recipes(self, recipe_title: str, num_results: int) -> List[Dict[str, Any]]:
        """ãƒ¢ãƒƒã‚¯ãƒ¬ã‚·ãƒ”ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ"""
        import random
        
        # ãƒ¢ãƒƒã‚¯ãƒ¬ã‚·ãƒ”ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ
        available_recipes = self.mock_recipes.copy()
        random.shuffle(available_recipes)
        
        # è¦æ±‚ã•ã‚ŒãŸæ•°ã ã‘è¿”ã™
        return available_recipes[:num_results]
    
    def _build_recipe_query(self, recipe_title: str) -> str:
        """ãƒ¬ã‚·ãƒ”æ¤œç´¢ç”¨ã®ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰"""
        # è¤‡æ•°ã‚µã‚¤ãƒˆã‚’å¯¾è±¡ã¨ã—ãŸæ¤œç´¢ã‚¯ã‚¨ãƒª
        sites_query = " OR ".join([f"site:{site}" for site in self.recipe_sites.keys()])
        return f"({sites_query}) {recipe_title} ãƒ¬ã‚·ãƒ”"
    
    def _parse_search_results(self, items: List[Dict]) -> List[Dict[str, Any]]:
        """æ¤œç´¢çµæœã‚’è§£æãƒ»æ•´å½¢"""
        recipes = []
        
        for item in items:
            # ã‚µã‚¤ãƒˆåã‚’ç‰¹å®š
            site_name = self._identify_site(item.get('link', ''))
            
            recipe = {
                'title': item.get('title', ''),
                'url': item.get('link', ''),
                'description': item.get('snippet', ''),
                'site': site_name,
                'source': self.recipe_sites.get(site_name, 'Unknown')
            }
            
            recipes.append(recipe)
        
        return recipes
    
    def _identify_site(self, url: str) -> str:
        """URLã‹ã‚‰ã‚µã‚¤ãƒˆåã‚’ç‰¹å®š"""
        for site in self.recipe_sites.keys():
            if site in url:
                return site
        return 'other'


class _PerplexitySearchClient:
    """Perplexity APIã‚’ä½¿ç”¨ã—ãŸãƒ¬ã‚·ãƒ”æ¤œç´¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.api_key = os.getenv('PERPLEXITY_API_KEY')
        
        if not self.api_key:
            raise ValueError("PERPLEXITY_API_KEY is required")
        
        self.api_url = "https://api.perplexity.ai/chat/completions"
        
        # å¯¾å¿œã‚µã‚¤ãƒˆã®å®šç¾©
        self.recipe_sites = {
            'cookpad.com': 'Cookpad',
            'kurashiru.com': 'ã‚¯ãƒ©ã‚·ãƒ«',
            'recipe.rakuten.co.jp': 'æ¥½å¤©ãƒ¬ã‚·ãƒ”',
            'delishkitchen.tv': 'ãƒ‡ãƒªãƒƒã‚·ãƒ¥ã‚­ãƒƒãƒãƒ³'
        }
    
    async def search_recipes(self, recipe_title: str, num_results: int = 5) -> List[Dict[str, Any]]:
        """ãƒ¬ã‚·ãƒ”æ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆPerplexity APIä½¿ç”¨ï¼‰"""
        logger.debug(f"ğŸ” [PERPLEXITY] Searching recipes")
        logger.debug(f"ğŸ” [PERPLEXITY] Recipe title: {recipe_title}")
        
        try:
            # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
            query = self._build_recipe_query(recipe_title)
            
            # Perplexity APIã‚’å‘¼ã³å‡ºã—
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "sonar",
                "messages": [
                    {
                        "role": "system",
                        "content": "ã‚ãªãŸã¯ãƒ¬ã‚·ãƒ”æ¤œç´¢ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¬ã‚·ãƒ”ã®URLã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
                    },
                    {
                        "role": "user",
                        "content": query
                    }
                ],
                "max_tokens": 1000,
                "temperature": 0.2
            }
            
            response = requests.post(
                self.api_url,
                headers=headers,
                json=payload,
                timeout=30
            )
            
            # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è©³ç´°ã‚’å–å¾—
            if response.status_code != 200:
                error_detail = response.text
                logger.error(f"âŒ [PERPLEXITY] API Error {response.status_code}: {error_detail}")
                logger.error(f"âŒ [PERPLEXITY] Request payload: {payload}")
                response.raise_for_status()
            
            result = response.json()
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰URLã‚’æŠ½å‡ºï¼ˆéåŒæœŸã§ç”»åƒã‚‚å–å¾—ï¼‰
            recipes = await self._parse_perplexity_response(result, recipe_title, num_results)
            
            logger.debug(f"âœ… [PERPLEXITY] Found recipes")
            logger.debug(f"ğŸ“Š [PERPLEXITY] Found {len(recipes)} recipes")
            return recipes
            
        except Exception as e:
            logger.error(f"âŒ [PERPLEXITY] Search error: {e}")
            return []
    
    def _build_recipe_query(self, recipe_title: str) -> str:
        """ãƒ¬ã‚·ãƒ”æ¤œç´¢ç”¨ã®ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰"""
        # è¤‡æ•°ã‚µã‚¤ãƒˆã‚’å¯¾è±¡ã¨ã—ãŸæ¤œç´¢ã‚¯ã‚¨ãƒª
        sites = "ã¾ãŸã¯".join(self.recipe_sites.keys())
        return f"{recipe_title} ãƒ¬ã‚·ãƒ” {sites} ã®URLã‚’æ•™ãˆã¦ãã ã•ã„ã€‚URLã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"
    
    async def _parse_perplexity_response(self, response: Dict, recipe_title: str, num_results: int) -> List[Dict[str, Any]]:
        """Perplexity APIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æãƒ»æ•´å½¢ï¼ˆç”»åƒURLã‚‚å–å¾—ï¼‰"""
        recipes = []
        
        try:
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
            choices = response.get('choices', [])
            if not choices:
                logger.warning(f"âš ï¸ [PERPLEXITY] No choices in response")
                return recipes
            
            content = choices[0].get('message', {}).get('content', '')
            
            # URLã‚’æŠ½å‡ºï¼ˆæ­£è¦è¡¨ç¾ã§URLã‚’æ¤œç´¢ï¼‰
            url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
            urls = re.findall(url_pattern, content)
            
            # ãƒ¬ã‚·ãƒ”ã‚µã‚¤ãƒˆã®URLã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            recipe_urls = []
            for url in urls:
                for site in self.recipe_sites.keys():
                    if site in url:
                        recipe_urls.append(url)
                        break
            
            # é‡è¤‡ã‚’é™¤å»
            recipe_urls = list(dict.fromkeys(recipe_urls))
            
            # è¦æ±‚ã•ã‚ŒãŸæ•°ã ã‘å‡¦ç†ï¼ˆç”»åƒURLã‚‚ä¸¦åˆ—å–å¾—ï¼‰
            import asyncio
            recipe_data_list = []
            image_tasks = []
            
            for url in recipe_urls[:num_results]:
                site_name = self._identify_site(url)
                recipe_data_list.append({
                    'url': url,
                    'site_name': site_name
                })
                # ç”»åƒå–å¾—ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
                image_tasks.append(self._fetch_recipe_image(url))
            
            # ç”»åƒå–å¾—ã‚’ä¸¦åˆ—å®Ÿè¡Œ
            image_urls = await asyncio.gather(*image_tasks)
            
            # ãƒ¬ã‚·ãƒ”ãƒ‡ãƒ¼ã‚¿ã¨ç”»åƒURLã‚’çµåˆ
            for recipe_data, image_url in zip(recipe_data_list, image_urls):
                recipe = {
                    'title': recipe_title,
                    'url': recipe_data['url'],
                    'description': f'{recipe_title}ã®ãƒ¬ã‚·ãƒ”ï¼ˆPerplexityæ¤œç´¢ï¼‰',
                    'site': recipe_data['site_name'],
                    'source': self.recipe_sites.get(recipe_data['site_name'], 'Unknown'),
                    'image_url': image_url  # ç”»åƒURLã‚’è¿½åŠ 
                }
                recipes.append(recipe)
            
        except Exception as e:
            logger.error(f"âŒ [PERPLEXITY] Error parsing response: {e}")
        
        return recipes
    
    def _identify_site(self, url: str) -> str:
        """URLã‹ã‚‰ã‚µã‚¤ãƒˆåã‚’ç‰¹å®š"""
        for site in self.recipe_sites.keys():
            if site in url:
                return site
        return 'other'
    
    async def _fetch_recipe_image(self, url: str) -> Optional[str]:
        """
        ãƒ¬ã‚·ãƒ”ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒURLã‚’å–å¾—
        
        Args:
            url: ãƒ¬ã‚·ãƒ”ãƒšãƒ¼ã‚¸ã®URL
        
        Returns:
            ç”»åƒURLï¼ˆå–å¾—å¤±æ•—æ™‚ã¯Noneï¼‰
        """
        try:
            # HTMLã‚’å–å¾—
            response = requests.get(
                url, 
                timeout=5, 
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                }
            )
            response.raise_for_status()
            
            # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
            soup = BeautifulSoup(response.text, 'lxml')
            
            # ãƒ‡ãƒãƒƒã‚°: HTMLã®ä¸€éƒ¨ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆæœ€åˆã®1000æ–‡å­—ï¼‰
            logger.debug(f"ğŸ” [PERPLEXITY] HTML preview for {url}: {response.text[:1000]}")
            
            # 1. OGPç”»åƒã‚’å„ªå…ˆçš„ã«å–å¾—
            og_image = soup.find('meta', property='og:image')
            if og_image and og_image.get('content'):
                image_url = og_image['content']
                # ç›¸å¯¾URLã®å ´åˆã¯çµ¶å¯¾URLã«å¤‰æ›
                if image_url.startswith('//'):
                    image_url = 'https:' + image_url
                elif image_url.startswith('/'):
                    image_url = urljoin(url, image_url)
                logger.info(f"ğŸ–¼ï¸ [PERPLEXITY] Found OGP image for {url}: {image_url}")
                return image_url
            else:
                logger.debug(f"ğŸ” [PERPLEXITY] No OGP image found for {url}")
            
            # 2. Twitter Cardç”»åƒ
            twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
            if twitter_image and twitter_image.get('content'):
                image_url = twitter_image['content']
                if image_url.startswith('//'):
                    image_url = 'https:' + image_url
                elif image_url.startswith('/'):
                    image_url = urljoin(url, image_url)
                logger.debug(f"ğŸ–¼ï¸ [PERPLEXITY] Found Twitter image for {url}: {image_url}")
                return image_url
            
            # 3. ã‚¯ãƒ©ã‚·ãƒ«å°‚ç”¨: ç‰¹å®šã®ã‚¯ãƒ©ã‚¹åã®ç”»åƒã‚’å–å¾—
            if 'kurashiru.com' in url:
                logger.debug(f"ğŸ” [PERPLEXITY] Searching for Kurashiru image in {url}")
                # ã‚¯ãƒ©ã‚·ãƒ«ã®ãƒ¬ã‚·ãƒ”ç”»åƒã¯é€šå¸¸ã€ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚„dataå±æ€§ã«å«ã¾ã‚Œã‚‹
                # ã¾ãšã€ã™ã¹ã¦ã®imgã‚¿ã‚°ã‚’ç¢ºèª
                all_imgs = soup.find_all('img')
                logger.debug(f"ğŸ” [PERPLEXITY] Found {len(all_imgs)} img tags in Kurashiru page")
                
                # OGPç”»åƒãŒæ—¢ã«å–å¾—ã§ãã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆOGPãŒå„ªå…ˆï¼‰
                # ã‚¯ãƒ©ã‚·ãƒ«ã®ãƒ¬ã‚·ãƒ”ç”»åƒã¯é€šå¸¸ã€ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚„dataå±æ€§ã«å«ã¾ã‚Œã‚‹
                img_tag = soup.find('img', class_=lambda x: x and ('recipe-image' in str(x).lower() or 'main-image' in str(x).lower() or 'hero-image' in str(x).lower()))
                if not img_tag:
                    # data-srcå±æ€§ã‚‚ç¢ºèª
                    img_tag = soup.find('img', attrs={'data-src': True})
                if not img_tag:
                    # videoã‚¿ã‚°ã®posterå±æ€§ã‚‚ç¢ºèªï¼ˆã‚¯ãƒ©ã‚·ãƒ«ã¯å‹•ç”»ã‚µã‚¤ãƒˆï¼‰
                    video_tag = soup.find('video')
                    if video_tag and video_tag.get('poster'):
                        image_url = video_tag['poster']
                        if image_url.startswith('//'):
                            image_url = 'https:' + image_url
                        elif image_url.startswith('/'):
                            image_url = urljoin(url, image_url)
                        logger.info(f"ğŸ–¼ï¸ [PERPLEXITY] Found Kurashiru video poster for {url}: {image_url}")
                        return image_url
                if img_tag:
                    image_url = img_tag.get('src') or img_tag.get('data-src')
                    if image_url:
                        if image_url.startswith('//'):
                            image_url = 'https:' + image_url
                        elif image_url.startswith('/'):
                            image_url = urljoin(url, image_url)
                        logger.info(f"ğŸ–¼ï¸ [PERPLEXITY] Found Kurashiru image for {url}: {image_url}")
                        return image_url
                logger.debug(f"âš ï¸ [PERPLEXITY] No Kurashiru-specific image found for {url}")
            
            # 4. ãƒ‡ãƒªãƒƒã‚·ãƒ¥ã‚­ãƒƒãƒãƒ³å°‚ç”¨: ç‰¹å®šã®ã‚¯ãƒ©ã‚¹åã®ç”»åƒã‚’å–å¾—
            if 'delishkitchen.tv' in url:
                # ãƒ‡ãƒªãƒƒã‚·ãƒ¥ã‚­ãƒƒãƒãƒ³ã®ãƒ¬ã‚·ãƒ”ç”»åƒã‚’å–å¾—
                img_tag = soup.find('img', class_=lambda x: x and ('recipe-image' in str(x).lower() or 'main-image' in str(x).lower() or 'hero-image' in str(x).lower()))
                if not img_tag:
                    # data-srcå±æ€§ã‚‚ç¢ºèª
                    img_tag = soup.find('img', attrs={'data-src': True})
                if img_tag:
                    image_url = img_tag.get('src') or img_tag.get('data-src')
                    if image_url:
                        if image_url.startswith('//'):
                            image_url = 'https:' + image_url
                        elif image_url.startswith('/'):
                            image_url = urljoin(url, image_url)
                        logger.debug(f"ğŸ–¼ï¸ [PERPLEXITY] Found DelishKitchen image for {url}: {image_url}")
                        return image_url
            
            # 5. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®å¤§ããªç”»åƒã‚’å–å¾—ï¼ˆã‚¢ã‚¤ã‚³ãƒ³ã‚„ãƒ­ã‚´ã‚’é™¤å¤–ï¼‰
            img_tags = soup.find_all('img')
            for img in img_tags:
                src = img.get('src') or img.get('data-src')
                if src:
                    # ã‚¢ã‚¤ã‚³ãƒ³ã‚„ãƒ­ã‚´ã‚’é™¤å¤–
                    skip_keywords = ['icon', 'logo', 'avatar', 'button', 'badge', 'spinner', 'loading']
                    if not any(skip in src.lower() for skip in skip_keywords):
                        # ã‚µã‚¤ã‚ºãŒå¤§ããã†ãªç”»åƒã‚’å„ªå…ˆï¼ˆwidth/heightå±æ€§ã‚’ç¢ºèªï¼‰
                        width = img.get('width')
                        height = img.get('height')
                        if width and height:
                            try:
                                w = int(str(width).replace('px', ''))
                                h = int(str(height).replace('px', ''))
                                # å°ã•ã™ãã‚‹ç”»åƒã¯ã‚¹ã‚­ãƒƒãƒ—
                                if w < 100 or h < 100:
                                    continue
                            except ValueError:
                                pass
                        
                        if src.startswith('//'):
                            src = 'https:' + src
                        elif src.startswith('/'):
                            src = urljoin(url, src)
                        logger.debug(f"ğŸ–¼ï¸ [PERPLEXITY] Found fallback image for {url}: {src}")
                        return src
            
            logger.warning(f"âš ï¸ [PERPLEXITY] No image found for {url}")
            return None
            
        except requests.exceptions.Timeout:
            logger.warning(f"âš ï¸ [PERPLEXITY] Timeout while fetching image from {url}")
            return None
        except requests.exceptions.RequestException as e:
            logger.warning(f"âš ï¸ [PERPLEXITY] Request error while fetching image from {url}: {e}")
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ [PERPLEXITY] Failed to fetch image from {url}: {e}")
            return None


def prioritize_recipes(recipes: List[Dict]) -> List[Dict]:
    """ãƒ¬ã‚·ãƒ”ã‚’å„ªå…ˆé †ä½ã§ã‚½ãƒ¼ãƒˆ"""
    priority_order = ['cookpad.com', 'kurashiru.com', 'recipe.rakuten.co.jp', 'delishkitchen.tv']
    
    def get_priority(recipe):
        site = recipe.get('site', '')
        try:
            return priority_order.index(site)
        except ValueError:
            return len(priority_order)
    
    return sorted(recipes, key=get_priority)


def filter_recipe_results(recipes: List[Dict]) -> List[Dict]:
    """ãƒ¬ã‚·ãƒ”çµæœã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    filtered = []
    
    for recipe in recipes:
        # åŸºæœ¬çš„ãªæ¤œè¨¼
        if recipe.get('title') and recipe.get('url'):
            # ãƒ¬ã‚·ãƒ”ã‚µã‚¤ãƒˆã‹ã©ã†ã‹ã‚’ç¢ºèª
            if recipe.get('site') in ['cookpad.com', 'kurashiru.com', 'recipe.rakuten.co.jp', 'delishkitchen.tv']:
                filtered.append(recipe)
    
    return filtered


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Google Searchï¼‰
# ç’°å¢ƒå¤‰æ•° USE_PERPLEXITY_SEARCH ã§Perplexityã«åˆ‡ã‚Šæ›¿ãˆå¯èƒ½
USE_PERPLEXITY_SEARCH = os.getenv('USE_PERPLEXITY_SEARCH', 'False').lower() in ('true', '1', 'yes')

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
try:
    if USE_PERPLEXITY_SEARCH:
        search_client = _PerplexitySearchClient()
        logger.info("ğŸ” [WEB] Using Perplexity Search (global)")
    else:
        search_client = _GoogleSearchClient()
        logger.info("ğŸ” [WEB] Using Google Search (global)")
except Exception as e:
    logger.warning(f"âš ï¸ [WEB] Failed to initialize search client: {e}, falling back to Google Search")
    search_client = _GoogleSearchClient()

# æ¤œç´¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆå†åˆ©ç”¨ã®ãŸã‚ï¼‰
_google_search_client = None
_perplexity_search_client = None


def get_search_client(menu_source: str = "mixed", use_perplexity: bool = None) -> Any:
    """
    æ¤œç´¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆmenu_sourceã«åŸºã¥ã„ã¦å‹•çš„ã«é¸æŠï¼‰
    
    Args:
        menu_source: ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ã‚½ãƒ¼ã‚¹ï¼ˆ"llm", "rag", "mixed"ï¼‰
        use_perplexity: å¼·åˆ¶çš„ã«Perplexityã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆNoneã®å ´åˆã¯menu_sourceã«åŸºã¥ã„ã¦æ±ºå®šï¼‰
    
    Returns:
        æ¤œç´¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    global _google_search_client, _perplexity_search_client
    
    # ç’°å¢ƒå¤‰æ•°ã§å…¨ä½“ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹å ´åˆ
    if USE_PERPLEXITY_SEARCH:
        if _perplexity_search_client is None:
            try:
                _perplexity_search_client = _PerplexitySearchClient()
            except Exception as e:
                logger.warning(f"âš ï¸ [WEB] Failed to initialize Perplexity client: {e}, falling back to Google Search")
                if _google_search_client is None:
                    _google_search_client = _GoogleSearchClient()
                return _google_search_client
        return _perplexity_search_client
    
    # use_perplexityãŒæ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
    if use_perplexity is True:
        if _perplexity_search_client is None:
            try:
                _perplexity_search_client = _PerplexitySearchClient()
            except Exception as e:
                logger.warning(f"âš ï¸ [WEB] Failed to initialize Perplexity client: {e}, falling back to Google Search")
                if _google_search_client is None:
                    _google_search_client = _GoogleSearchClient()
                return _google_search_client
        return _perplexity_search_client
    
    # menu_sourceãŒ"llm"ã®å ´åˆã¯Perplexityã‚’ä½¿ç”¨
    if menu_source == "llm":
        logger.debug(f"ğŸ” [WEB] menu_source='llm' detected, attempting to use Perplexity Search")
        if _perplexity_search_client is None:
            try:
                _perplexity_search_client = _PerplexitySearchClient()
                logger.info("âœ… [WEB] Perplexity Search client initialized successfully for LLM proposals")
            except ValueError as e:
                logger.error(f"âŒ [WEB] Perplexity API key not configured: {e}")
                logger.warning(f"âš ï¸ [WEB] Falling back to Google Search (may use mock data)")
                if _google_search_client is None:
                    _google_search_client = _GoogleSearchClient()
                return _google_search_client
            except Exception as e:
                logger.error(f"âŒ [WEB] Failed to initialize Perplexity client: {e}")
                logger.warning(f"âš ï¸ [WEB] Falling back to Google Search (may use mock data)")
                if _google_search_client is None:
                    _google_search_client = _GoogleSearchClient()
                return _google_search_client
        logger.debug(f"ğŸ” [WEB] Returning Perplexity Search client for LLM proposals")
        return _perplexity_search_client
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Google Search
    if _google_search_client is None:
        _google_search_client = _GoogleSearchClient()
    return _google_search_client
